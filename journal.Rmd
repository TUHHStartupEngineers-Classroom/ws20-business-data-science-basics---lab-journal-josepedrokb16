---
title: "Journal (reproducible report)"
author: "Jose Pedro Kitajima Borges"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```


# Challenge - Intro to the tidyverse

## Business Case and Challenge

The goal is to analyze the sales of bikes through stores in Germany. Data is organized as "sales by location" and "sales by year and location".

Bike sales data divided into multiple data sets. Entity-relationship diagrams describe and define the data models. They explain the logical structure of the database.

## Importing data
The first step is to import the necessary libraries

```{r, eval = TRUE}
library(tidyverse)
library(readxl)
library(lubridate)
```

We then read all of the Excel files
```{r, eval = TRUE}
bikes_tbl <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
```

## Data Organization
The next step is to combine all of the tables into one concise table
```{r, eval = TRUE}
bike_orderlines_joined_tbl <- left_join(orderlines_tbl, bikes_tbl, by = c("product.id"="bike.id")) %>% left_join(bikeshops, by = c("customer.id" = "bikeshop.id"))
bike_orderlines_joined_tbl
```

We now organize the data in the table to give us better insights

```{r, eval = TRUE}
bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%
  separate(col = category,
           into = c("category.1","category.2","category.3"),
           sep = " - ") %>%
  mutate(total.price = price*quantity) %>%
  select(-...1, -gender) %>%
  select(-ends_with(".id")) %>%
  bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>%
  
  select(order.id, contains("order"), contains("model"), contains("category"), price, quantity, total.price, everything()) %>%
  rename(bikeshop = name) %>%
  set_names(names(.) %>% str_replace_all("\\.","_"))
bike_orderlines_wrangled_tbl
```

Now we have to separate the location column into city and State

```{r, eval = TRUE}
bike_orderlines_wrangled_tbl <- bike_orderlines_wrangled_tbl %>% separate(col = location,
                                          into = c("City", "State"),
                                          sep = ", ")
```

Now we analyze the above table based on sales per State

```{r, eval = TRUE}
sales_by_state_tbl <- bike_orderlines_wrangled_tbl %>%
  select(State, total_price) %>%
  group_by(State) %>%
  summarise(sales = sum(total_price)) %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".",
                                     decimal.mark = ",",
                                     prefix = "",
                                     suffix = " €"))
sales_by_state_tbl
```
## Data Visualization

We now plot the data from the table to better visualize it

```{r, eval = TRUE}
sales_by_state_tbl %>%
  ggplot(aes(x = State, y = sales)) + 
  geom_col(fill = "#2DC6D6")  +
  geom_smooth(method = "lm", se = FALSE) + 
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) + 
  labs(title = "Revenue per State",
       subtitle = "Upward Trend",
       x = "",
       y = "Revenue") +
  theme(axis.text.x = element_text(angle=45, hjust = 1))

```

From the plot above, it is clear that the State where the revenue was bigger is North Rhine-Westphalia.

We now move on to analyze the sales by location and year.
We first create a new table containing only the important information. This new table should contain the year, revenue, and location.
```{r, eval = TRUE}
sales_by_location_year_tbl <- bike_orderlines_wrangled_tbl %>%
  select(order_date, State, total_price) %>%
  mutate(Year = year(order_date)) %>%
  group_by(State, Year) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".",
                                     decimal.mark = ",",
                                     prefix = "",
                                     suffix = " €"))
glimpse(sales_by_location_year_tbl)
  
```

We finally analyze the data with the help of column plots. There are 12 states with bike stores, meaning 12 plots.
```{r, eval = TRUE}
sales_by_location_year_tbl %>%
  ggplot(aes(x = Year, y = sales, fill = State)) +
  geom_col() + 
  facet_wrap(~ State) + 
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(title = "Revenue by year and main State",
       fill = "Main category") + 
  theme(axis.text.x = element_text(angle=45, hjust = 1))
```

# Challenge - Data Acquisition

## Getting Data from an Arbitrary API

We need to get data via an API. For this example, I will be fetching data from a Brazilian API that provides information about car prices sold in Brazil

```{r, eval = TRUE}
# import the httr library
library(httr)

# request access the the Brazilian car API
response <- GET("https://parallelum.com.br/fipe/api/v1/carros/marcas/39/modelos") %>%
  # Go the the folder containing the Mercedes-Benz models
  content(as = "text") %>% fromJSON() %>% .$modelos %>% as.tibble() %>% select(nome) %>%     rename("Mercedes Model" = nome)

print(response, n = 10)


```

## Creating a Database from Canyon's Competitor
We now scrape the website of one of the competitors of Canyon Rose Bikes. We create a small database that contains the model names, and the prices for at least one category.

Let's first import all of the necessary libraries

```{r, eval = TRUE}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing

```

Now we read the html code from the website

```{r, eval = TRUE}
# provide the home URL and read it
url_home          <- "https://www.rosebikes.de/"
html_home <- read_html(url_home)
```

we then scrape the models of the bikes
```{r, eval = TRUE}
# Create a new table with the categories
rose_bike_category <- html_home %>% 
  # Go to the node where the category names are located
  html_nodes(css = ".header-mobile-menu-item__title") %>% html_text() %>%
  # Extract the unecessary symbols
  str_extract(pattern = "(?<=\\n).*(?=\\n)") %>%
  # slice the rows that don't have a category
  as.tibble() %>% slice(3:11) %>%
  # Create a new column for the specific category URLs
  mutate(model_url = str_glue("{url_home}fahrräder/{value}"))

rose_bike_category
```

The next step is to scrape the models of the first category "MTB" 
```{r, eval = TRUE}
# Create a new table with the products of the MTB category
# We use the URL from the first row of the previous table
rose_bike_models <- rose_bike_category$model_url[1]  %>% 
  read_html() %>%
  # Go to the node that contains the product name
  html_nodes(css = ".catalog-category-bikes__title-text") %>% html_text() %>%
  # Extract the unecessary characters
  str_extract(pattern = "(?<=\\n).*(?=\\n)") %>%
  as.tibble() %>%
  rename("MTB Models" = value) 

# Create a new column with an ID for future merging
rose_bike_models$ID <- seq.int(nrow(rose_bike_models))
rose_bike_models <- select(rose_bike_models, ID, "MTB Models")

rose_bike_models

```
We now get a table with the prices of each MTB model 
```{r, eval = TRUE}
# We create a new table with the prices of the individual MTB bikes
rose_bike_MTB_price <- rose_bike_category$model_url[1]  %>% 
  read_html() %>%
  # We go to the node containing the prices
  html_nodes(css = ".catalog-category-bikes__price-title") %>% html_text() %>%
  # We get rid of unecessary characters
  stringr::str_extract(pattern = "(?<=ab ).*?(?=\\n)") %>%
  as.tibble() %>%
  rename("MTB Models Price" = value) 
 
# We create a new column with bike IDs for future merging
rose_bike_MTB_price$ID <- seq.int(nrow(rose_bike_models))
rose_bike_MTB_price <- select(rose_bike_MTB_price, ID, "MTB Models Price")

rose_bike_MTB_price

```
We finally join the two tables in one database
```{r, eval = TRUE}
# We merge the two tables based on the ID
rose_bike <- rose_bike_models %>% left_join(rose_bike_MTB_price)

rose_bike
```

# Challenge - Data Wrangling

## Part 1 - Patent Dominance
What US company / corporation has the most patents? List the 10 US companies with the most assigned/granted patents.

We obtain data from the United States Patent and Trademark Office.
The first data set to get is the one containing the assignees.

```{r, eval = FALSE}
# import the vroom library
library(vroom)
library(data.table)
library(dplyr)
# Get the assignee dataset
# define the column names and types based on the Excel spreadsheet
col_types <- list(
  id = col_character(),
  type = col_double(),
  name_first = col_character(),
  name_last = col_character(),
  organization = col_character()

)

# import file patent.tsv
assignee_tbl <- vroom(
            file       = "assignee.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```
For this project, we also need the name of the companies that issued the patent. These are not included in the first table. We must therefore download and import the table "patent_assignee.tsv" that contains the organization name. We will later link it to the "assignee.tsv" table by using the patent id.
```{r, eval = FALSE}
# Get the "patent_assignee" data set
# define the column names and types based on the Excel spreadsheet
col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_character()

)

# import file patent.tsv
patent_assignee_tbl <- vroom(
            file       = "patent_assignee.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```

We now convert both tables to a data.table format and merge them based on the assignee Id.
```{r, eval = FALSE}
# convert the data frame format to the data.table format
setDT(assignee_tbl)
setDT(patent_assignee_tbl)

# We rename the id column of the assignee_tbl, so that it has the same name as the assignee_id column in the patent_assignee_tbl. We also select only the important columns

 assignee_tbl <- assignee_tbl[, .(assignee_id = id, type, organization)]

```
We now merge the two tables based on the same variable "assignee_id"
```{r, eval = FALSE}
# Create a new table by merging the existing tables through the commnon column "assignee_id"
combined_patent_tbl <- assignee_tbl %>%
  left_join(patent_assignee_tbl, by = "assignee_id")

# We can extract only the two important columns of this table for this first analysis, namely the organization name and the patent ID columns
org_pat_tbl_copy <- combined_patent_tbl
org_pat_tbl <- combined_patent_tbl[, .(patent_id), by = organization]


rm(assignee_tbl)
rm(patent_assignee_tbl)
```
Now we can simply count the number of occurrencies of each organization name. This will give the number of patents issued by such organization.

```{r, eval = FALSE}
org_pat_tbl <- org_pat_tbl[!is.na(organization), .(number_of_patents = .N), by = organization]

# we then arrange in decreasing format and select the top 10 companies with the most patents.
top_10_pat_org <- org_pat_tbl %>% ungroup() %>% arrange(desc(number_of_patents)) %>% slice(1:10)

write_rds(top_10_pat_org, "top_10_pat_org.rds")

rm(org_pat_tbl)
```

we now display the results of the top 10 companies in terms of number of patents

```{r, eval = TRUE}
read_rds("top_10_pat_org.rds")
```

## Part 2 - Recent patent acitivity
What US company had the most patents granted in 2019? List the top 10 companies with the most new granted patents for 2019.

For this part of the challenge, we also need to load the data set "patent.tsv"
```{r, eval = FALSE}
# prepare column names and type
col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)

# import dataset
patent_tbl <- vroom(
            file       = "patent.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```

We can extract only the two most important columns for this part of the challenge: the "patent id" and the "date"

```{r, eval = FALSE}
# set table as data.table type
patent_short_table <- patent_tbl %>% select(id,date)
setDT(patent_short_table)
rm(patent_tbl)
# we rename the id column to use it as a key with the previous table

patent_short_table <- patent_short_table[, .(patent_id = id, date)]

# now we merge it with the previous combined table

combined_patent_date_tbl <- combined_patent_tbl %>%
  left_join(patent_short_table, by = "patent_id")

# Now we select the rows that are valid and that were issued in the year 2019. We count the number of occurrences for these patents in 2019 for each organization. We finally order them and select the top 10

combined_patent_date_tbl <- combined_patent_date_tbl[!is.na(date), .(patent_year = lubridate::year(date)), by = organization][
  patent_year == 2019][!is.na(organization)
    , .(num_patents_2019 = .N), by = organization
  ][
    order(num_patents_2019, decreasing = TRUE)
  ][
    1:10
  ]

write_rds(combined_patent_date_tbl, "num_patents_2019.rds")
```

we now print the results of the Top 10 companies with most patents in 2019.

```{r, eval = TRUE}
read_rds("num_patents_2019.rds")
```

## Part 3 -  Innovation in Tech: 
What is the most innovative tech sector? For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?
```{r, eval = FALSE}
# for this part of the challenge, we are going to need another data set "uspc.tsv"
# prepare column names and type
col_types <- list(
  uuid = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_character(),
  sequence = col_double()

)

# import data set
uspc_tbl <- vroom(
            file       = "uspc.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```

We now reference a copy of the merged table, done for the first step of this challenge. We build a table that contains the organization name, the patent_id and the mainclass_id.

```{r, eval = FALSE}
# set them as data.table
setDT(uspc_tbl)
setDT(org_pat_tbl_copy)

# we first collect only the important part of the org_pat_tbl_copy table, namely the organization name and the patent_id.
org_pat_tbl_copy <- org_pat_tbl_copy[!is.na(location_id), .(patent_id, organization)]

# we now filter the uspc table to get just the patent_id and the mainclass_id

uspc_pat_main_id_tbl <- uspc_tbl[, .(patent_id, mainclass_id)]

```

We now merge the two tables based on the patent_id
```{r, eval = FALSE}
# Create a new table by merging the existing tables through the commnon column "patent_id"
comb_patent_mainclass_tbl <- org_pat_tbl_copy %>%
  left_join(uspc_pat_main_id_tbl, by = "patent_id")

# filter out those companies that do not have a mainclass_id assigned
comb_patent_mainclass_tbl <- comb_patent_mainclass_tbl[!is.na(mainclass_id), 
                                                       .(mainclass_id), by = organization]

```

We can refer to the first part of the challenge and only select the companies among the top 10 companies in total number of patents.

```{r, eval = FALSE}
library(tidyr)
# We gather the top 10 companies in total number of patents from part one of the challenge
top_10_pat_org_copy <- top_10_pat_org[,.(organization)]

# we now merge this table above with the mainclass table and filter out the na values. This is done by using the key method. We then count the number of times a given patent mainclass_id appeared in these top 10 companies.

setkey(comb_patent_mainclass_tbl, organization)
setkey(top_10_pat_org_copy, organization)

top_10_main_id <- comb_patent_mainclass_tbl[top_10_pat_org_copy, on = "organization"][ , .N, by = mainclass_id
][
  order(N, decreasing = TRUE)
][
  1:5, .(`Top 5 Classes` = mainclass_id, `Number of Patents` = N)
]

write_rds(top_10_main_id, "top_10_mainclass.rds")
```

We print the results showing the top 5 main classes Ids from the top 10 companies in the world based on number of patents issued.
```{r, eval = TRUE}
print(read_rds("top_10_mainclass.rds"))

```


# Challenge - Data Visualization

We will deal with the Covid-19  data and use tidyverse to wrangle the data.


```{r, eval = TRUE}
# Let's first load the libraries and the dataset
library(tidyverse)
covid_data_tbl <- read_csv("https://opendata.ecdc.europa.eu/covid19/casedistribution/csv")

```

## Part 1 

The goal is to map the time course of the cumulative Covid-19 cases.

```{r, eval = TRUE}
# Let's first filter out the countries that are relevant for this analysis

covid_analysis <- covid_data_tbl %>% 
  
  select(dateRep, cases, countriesAndTerritories) %>%
  filter(countriesAndTerritories %in% c("Germany", "United_Kingdom", "France", "Spain", "United_States_of_America")) %>%

# We now order the rows based on country and date, and create a new column containing the cumulative sum of the cases for each country
  
  group_by(countriesAndTerritories) %>%
  mutate(date = dmy(dateRep)) %>%
  arrange(countriesAndTerritories, date) %>%

  mutate(Cumulative_Sum = cumsum(cases)) %>%
  filter(year(date) == 2020) %>%
  mutate(month = month(date, label = TRUE)) %>% 
  mutate(sum_format = scales::dollar(Cumulative_Sum, big.mark = ".",
                                     decimal.mark    = ",",
                                     prefix          = ""))



# The Next Step is to begin visualizing the date

covid_analysis %>%
  
  # We determine the X and Y axis, as well as differentiatte the different curves by country
  
  ggplot(aes(date, Cumulative_Sum, color = countriesAndTerritories)) +
  
  geom_line(aes(color = countriesAndTerritories), size = 1) +
  
  # We adjust the X axis to show only the months in 2020. We also make the notation in the Y axis easier to read.

  scale_x_date(date_breaks = "1 month", date_labels = "%b") +


  scale_y_continuous(labels = scales::dollar_format(scale = 1e-6,
                                                      prefix = "",
                                                      suffix = " M"), breaks = seq(0,15e6, by = 2.5e6)) +


  # We create a label on the last data point for the United States Curve, as shown in the challenge objective
  
geom_label(data = filter(covid_analysis, Cumulative_Sum ==last(Cumulative_Sum), countriesAndTerritories %in% "United_States_of_America"), aes(label = sum_format, fill = countriesAndTerritories), hjust = "inward",
               size  = 3,
               color = RColorBrewer::brewer.pal(n = 11, name = "RdBu")[11], show.legend = FALSE) +

  # We write all of the titles of the axis
labs(
      title = "COVID-19 Confirmed Cases Worldwide",
      x = "Year 2020",
      y = "Cumulative Cases"

    ) +

  # We change the default coloring and position of the legend
theme_economist() +

theme(
      axis.text.x = element_text(angle = 45, hjust = -1),
      plot.title = element_text(face = "bold"),
      plot.caption = element_text(face = "bold.italic"),
      legend.position = "bottom",
      legend.direction = "vertical") +

scale_color_viridis_d(option = "D")

  
```

## Part 2

We want now to visualize the mortality rate (deaths / population).

```{r, eval = TRUE}
# We access the longitudinal and lateral data

world <- map_data("world")

# We now adjust the current table so that the countries all have the same name

covid_map <- covid_data_tbl%>%
   mutate(across(countriesAndTerritories, str_replace_all, "_", " ")) %>%
   mutate(countriesAndTerritories = case_when(

    countriesAndTerritories == "United Kingdom" ~ "UK",
    countriesAndTerritories == "United States of America" ~ "USA",
    countriesAndTerritories == "Czechia" ~ "Czech Republic",
    TRUE ~ countriesAndTerritories

  )) %>%
  
  # We select the important columns, and create a new column containing the death rate per population
  mutate(region = countriesAndTerritories) %>%
  select(region, deaths, popData2019) %>%
  group_by(region) %>%
  summarise(deathRate = sum(deaths)/popData2019) %>%
  distinct(region, .keep_all = TRUE) %>%
  mutate(death_rate_formatted = scales::percent(deathRate, accuracy = 0.001 ))

# Now we can join the two table

covid_map <- covid_map %>%
  left_join(world, by = "region") %>%
  distinct(region, .keep_all = TRUE) %>%
  mutate(`Mortality Rate` = deathRate)

# We now put the this data into the map argument of geom_map()

covid_map %>% ggplot() + 
  geom_map(aes(x = long, y = lat, map_id = region, fill = `Mortality Rate`, label = death_rate_formatted), map = world) +
  
  # We now do the formatting so that it looks nice

  theme_minimal() + 
  
  # Writing the labels
  labs(
    
    title = "Confirmed COVID-19 deaths relative to the size of the population",
    subtitle = "More than 1.2 Million confirmed Covid-19 deaths worldwide",
    x = "",
    y = "",
    caption = "Date: 05/12/2020"
  ) + 
  
  # changing the legend colors, and format to percentage style
  scale_fill_continuous(labels = scales::percent_format(accuracy = 0.001), low = "#a52a2a", high = "black") 

```

