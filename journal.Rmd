---
title: "Journal (reproducible report)"
author: "Jose Pedro Kitajima Borges"
date: "2020-11-05"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: true
    toc_depth: 3
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

**IMPORTANT:** You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.

This is an `.Rmd` file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a \# in front of your text, it will create a top level-header.

# My first post

Last compiled: `r Sys.Date()`

Notice that whatever you define as a top level header, automatically gets put into the table of contents bar on the left. 

## Second level header

You can add more headers by adding more hashtags. These won't be put into the table of contents

### third level header

Here's an even lower level header

# My second post (note the order)

Last compiled: `r Sys.Date()`

I'm writing this tutorial going from the top down. And, this is how it will be printed. So, notice the second post is second in the list. If you want your most recent post to be at the top, then make a new post starting at the top. If you want the oldest first, do, then keep adding to the bottom

# Adding R stuff

So far this is just a blog where you can write in plain text and serve your writing to a webpage. One of the main purposes of this lab journal is to record your progress learning R. The reason I am asking you to use this process is because you can both make a website, and a lab journal, and learn R all in R-studio. This makes everything really convenient and in the same place. 

So, let's say you are learning how to make a histogram in R. For example, maybe you want to sample 100 numbers from a normal distribution with mean = 0, and standard deviation = 1, and then you want to plot a histogram. You can do this right here by using an r code block, like this:

```{r}
samples <- rnorm(100, mean=0, sd=1)
hist(samples)
```

When you knit this R Markdown document, you will see that the histogram is printed to the page, along with the R code. This document can be set up to hide the R code in the webpage, just delete the comment (hashtag) from the cold folding option in the yaml header up top. For purposes of letting yourself see the code, and me see the code, best to keep it the way that it is. You'll learn that all of these things and more can be customized in each R code block.


```{r, eval = TRUE}
numbers <- 1:1000

# This will print the first 10 elements of the vector numbers
numbers[1:10]

# This will plot a histogram of 100 random elements of the vector numbers
hist(sample(numbers, 100, replace = T))
```

# Challenge 2 - Intro to the tidyverse

## Business Case and Challenge

The goal is to analyze the sales of bikes through stores in Germany. Data is organized as "sales by location" and "sales by year and location".

Bike sales data divided into multiple data sets. Entity-relationship diagrams describe and define the data models. They explain the logical structure of the database.

## Importing data
The first step is to import the necessary libraries

```{r, eval = TRUE}
library(tidyverse)
library(readxl)
library(lubridate)
```

We then read all of the Excel files
```{r, eval = TRUE}
bikes_tbl <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/bikes.xlsx")
orderlines_tbl <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/orderlines.xlsx")
bikeshops <- read_excel(path = "DS_101/00_data/01_bike_sales/01_raw_data/bikeshops.xlsx")
```

## Data Organization
The next step is to combine all of the tables into one concise table
```{r, eval = TRUE}
bike_orderlines_joined_tbl <- left_join(orderlines_tbl, bikes_tbl, by = c("product.id"="bike.id")) %>% left_join(bikeshops, by = c("customer.id" = "bikeshop.id"))
glimpse(bike_orderlines_joined_tbl)
```

We now organize the data in the table to give us better insights

```{r, eval = TRUE}
bike_orderlines_wrangled_tbl <- bike_orderlines_joined_tbl %>%
  separate(col = category,
           into = c("category.1","category.2","category.3"),
           sep = " - ") %>%
  mutate(total.price = price*quantity) %>%
  select(-...1, -gender) %>%
  select(-ends_with(".id")) %>%
  bind_cols(bike_orderlines_joined_tbl %>% select(order.id)) %>%
  
  select(order.id, contains("order"), contains("model"), contains("category"), price, quantity, total.price, everything()) %>%
  rename(bikeshop = name) %>%
  set_names(names(.) %>% str_replace_all("\\.","_"))
glimpse(bike_orderlines_wrangled_tbl)
```

Now we have to separate the location column into city and State

```{r, eval = TRUE}
bike_orderlines_wrangled_tbl <- bike_orderlines_wrangled_tbl %>% separate(col = location,
                                          into = c("City", "State"),
                                          sep = ", ")
```

Now we analyze the above table based on sales per State

```{r, eval = TRUE}
sales_by_state_tbl <- bike_orderlines_wrangled_tbl %>%
  select(State, total_price) %>%
  group_by(State) %>%
  summarise(sales = sum(total_price)) %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".",
                                     decimal.mark = ",",
                                     prefix = "",
                                     suffix = " €"))
sales_by_state_tbl
```
## Data Visualization

We now plot the data from the table to better visualize it

```{r, eval = TRUE}
sales_by_state_tbl %>%
  ggplot(aes(x = State, y = sales)) + 
  geom_col(fill = "#2DC6D6")  +
  geom_smooth(method = "lm", se = FALSE) + 
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) + 
  labs(title = "Revenue per State",
       subtitle = "Upward Trend",
       x = "",
       y = "Revenue") +
  theme(axis.text.x = element_text(angle=45, hjust = 1))

```

From the plot above, it is clear that the State where the revenue was bigger is North Rhine-Westphalia.

We now move on to analyze the sales by location and year.
We first create a new table containing only the important information. This new table should contain the year, revenue, and location.
```{r, eval = TRUE}
sales_by_location_year_tbl <- bike_orderlines_wrangled_tbl %>%
  select(order_date, State, total_price) %>%
  mutate(Year = year(order_date)) %>%
  group_by(State, Year) %>%
  summarise(sales = sum(total_price)) %>%
  ungroup() %>%
  mutate(sales_text = scales::dollar(sales, big.mark = ".",
                                     decimal.mark = ",",
                                     prefix = "",
                                     suffix = " €"))
glimpse(sales_by_location_year_tbl)
  
```

We finally analyze the data with the help of column plots. There are 12 states with bike stores, meaning 12 plots.
```{r, eval = TRUE}
sales_by_location_year_tbl %>%
  ggplot(aes(x = Year, y = sales, fill = State)) +
  geom_col() + 
  facet_wrap(~ State) + 
  scale_y_continuous(labels = scales::dollar_format(big.mark = ".", 
                                                    decimal.mark = ",", 
                                                    prefix = "", 
                                                    suffix = " €")) +
  labs(title = "Revenue by year and main State",
       fill = "Main category") + 
  theme(axis.text.x = element_text(angle=45, hjust = 1))
```

# Challenge 3 Data Acquisition

## Getting Data from an Arbitrary API

We need to get data via an API. For this example, I will be fetching data from a Brazilian API that provides information about car prices sold in Brazil

```{r, eval = TRUE}
# import the httr library
library(httr)

# request access the the Brazilian car API
response <- GET("https://parallelum.com.br/fipe/api/v1/carros/marcas/39/modelos") %>%
  # Go the the folder containing the Mercedes-Benz models
  content(as = "text") %>% fromJSON() %>% .$modelos %>% as.tibble() %>% select(nome) %>%     rename("Mercedes Model" = nome)

print(response, n = 10)


```

## Creating a Database from Canyon's Competitor
We now scrape the website of one of the competitors of Canyon Rose Bikes. We create a small database that contains the model names, and the prices for at least one category.

Let's first import all of the necessary libraries

```{r, eval = TRUE}
library(tidyverse) # Main Package - Loads dplyr, purrr, etc.
library(rvest)     # HTML Hacking & Web Scraping
library(xopen)     # Quickly opening URLs
library(jsonlite)  # converts JSON files to R objects
library(glue)      # concatenate strings
library(stringi)   # character string/text processing

```

Now we read the html code from the website

```{r, eval = TRUE}
# provide the home URL and read it
url_home          <- "https://www.rosebikes.de/"
html_home <- read_html(url_home)
```

we then scrape the models of the bikes
```{r, eval = TRUE}
# Create a new table with the categories
rose_bike_category <- html_home %>% 
  # Go to the node where the category names are located
  html_nodes(css = ".header-mobile-menu-item__title") %>% html_text() %>%
  # Extract the unecessary symbols
  str_extract(pattern = "(?<=\\n).*(?=\\n)") %>%
  # slice the rows that don't have a category
  as.tibble() %>% slice(3:11) %>%
  # Create a new column for the specific category URLs
  mutate(model_url = str_glue("{url_home}fahrräder/{value}"))

rose_bike_category
```

The next step is to scrape the models of the first category "MTB" 
```{r, eval = TRUE}
# Create a new table with the products of the MTB category
# We use the URL from the first row of the previous table
rose_bike_models <- rose_bike_category$model_url[1]  %>% 
  read_html() %>%
  # Go to the node that contains the product name
  html_nodes(css = ".catalog-category-bikes__title-text") %>% html_text() %>%
  # Extract the unecessary characters
  str_extract(pattern = "(?<=\\n).*(?=\\n)") %>%
  as.tibble() %>%
  rename("MTB Models" = value) 

# Create a new column with an ID for future merging
rose_bike_models$ID <- seq.int(nrow(rose_bike_models))
rose_bike_models <- select(rose_bike_models, ID, "MTB Models")

rose_bike_models

```
We now get a table with the prices of each MTB model 
```{r, eval = TRUE}
# We create a new table with the prices of the individual MTB bikes
rose_bike_MTB_price <- rose_bike_category$model_url[1]  %>% 
  read_html() %>%
  # We go to the node containing the prices
  html_nodes(css = ".catalog-category-bikes__price-title") %>% html_text() %>%
  # We get rid of unecessary characters
  stringr::str_extract(pattern = "(?<=ab ).*?(?=\\n)") %>%
  as.tibble() %>%
  rename("MTB Models Price" = value) 
 
# We create a new column with bike IDs for future merging
rose_bike_MTB_price$ID <- seq.int(nrow(rose_bike_models))
rose_bike_MTB_price <- select(rose_bike_MTB_price, ID, "MTB Models Price")

rose_bike_MTB_price

```
We finally join the two tables in one database
```{r, eval = TRUE}
# We merge the two tables based on the ID
rose_bike <- rose_bike_models %>% left_join(rose_bike_MTB_price)

rose_bike
```

# Challenge 4 - Data Wrangling

## Part 1 - Patent Dominance
What US company / corporation has the most patents? List the 10 US companies with the most assigned/granted patents.

We obtain data from the United States Patent and Trademark Office.
The first data set to get is the one containing the assignees.

```{r, eval = FALSE}
# import the vroom library
library(vroom)
library(data.table)
library(dplyr)
# Get the assignee dataset
# define the column names and types based on the Excel spreadsheet
col_types <- list(
  id = col_character(),
  type = col_double(),
  name_first = col_character(),
  name_last = col_character(),
  organization = col_character()

)

# import file patent.tsv
assignee_tbl <- vroom(
            file       = "assignee.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```
For this project, we also need the name of the companies that issued the patent. These are not included in the first table. We must therefore download and import the table "patent_assignee.tsv" that contains the organization name. We will later link it to the "assignee.tsv" table by using the patent id.
```{r, eval = FALSE}
# Get the "patent_assignee" data set
# define the column names and types based on the Excel spreadsheet
col_types <- list(
  patent_id = col_character(),
  assignee_id = col_character(),
  location_id = col_character()

)

# import file patent.tsv
patent_assignee_tbl <- vroom(
            file       = "patent_assignee.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```

We now convert both tables to a data.table format and merge them based on the assignee Id.
```{r, eval = FALSE}
# convert the data frame format to the data.table format
setDT(assignee_tbl)
setDT(patent_assignee_tbl)

# We rename the id column of the assignee_tbl, so that it has the same name as the assignee_id column in the patent_assignee_tbl. We also select only the important columns

 assignee_tbl <- assignee_tbl[, .(assignee_id = id, type, organization)]

```
We now merge the two tables based on the same variable "assignee_id"
```{r, eval = FALSE}
# Create a new table by merging the existing tables through the commnon column "assignee_id"
combined_patent_tbl <- assignee_tbl %>%
  left_join(patent_assignee_tbl, by = "assignee_id")

# We can extract only the two important columns of this table for this first analysis, namely the organization name and the patent ID columns
org_pat_tbl_copy <- combined_patent_tbl
org_pat_tbl <- combined_patent_tbl[, .(patent_id), by = organization]


rm(assignee_tbl)
rm(patent_assignee_tbl)
```
Now we can simply count the number of occurrencies of each organization name. This will give the number of patents issued by such organization.

```{r, eval = FALSE}
org_pat_tbl <- org_pat_tbl[!is.na(organization), .(number_of_patents = .N), by = organization]

# we then arrange in decreasing format and select the top 10 companies with the most patents.
top_10_pat_org <- org_pat_tbl %>% ungroup() %>% arrange(desc(number_of_patents)) %>% slice(1:10)

write_rds(top_10_pat_org, "top_10_pat_org.rds")

rm(org_pat_tbl)
```

we now display the results of the top 10 companies in terms of number of patents

```{r, eval = TRUE}
read_rds("top_10_pat_org.rds")
```

## Part 2 - Recent patent acitivity
What US company had the most patents granted in 2019? List the top 10 companies with the most new granted patents for 2019.

For this part of the challenge, we also need to load the data set "patent.tsv"
```{r, eval = FALSE}
# prepare column names and type
col_types <- list(
  id = col_character(),
  type = col_character(),
  number = col_character(),
  country = col_character(),
  date = col_date("%Y-%m-%d"),
  abstract = col_character(),
  title = col_character(),
  kind = col_character(),
  num_claims = col_double(),
  filename = col_character(),
  withdrawn = col_double()
)

# import dataset
patent_tbl <- vroom(
            file       = "patent.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```

We can extract only the two most important columns for this part of the challenge: the "patent id" and the "date"

```{r, eval = FALSE}
# set table as data.table type
patent_short_table <- patent_tbl %>% select(id,date)
setDT(patent_short_table)
rm(patent_tbl)
# we rename the id column to use it as a key with the previous table

patent_short_table <- patent_short_table[, .(patent_id = id, date)]

# now we merge it with the previous combined table

combined_patent_date_tbl <- combined_patent_tbl %>%
  left_join(patent_short_table, by = "patent_id")

# Now we select the rows that are valid and that were issued in the year 2019. We count the number of occurrences for these patents in 2019 for each organization. We finally order them and select the top 10

combined_patent_date_tbl <- combined_patent_date_tbl[!is.na(date), .(patent_year = lubridate::year(date)), by = organization][
  patent_year == 2019][!is.na(organization)
    , .(num_patents_2019 = .N), by = organization
  ][
    order(num_patents_2019, decreasing = TRUE)
  ][
    1:10
  ]

write_rds(combined_patent_date_tbl, "num_patents_2019.rds")
```

we now print the results of the Top 10 companies with most patents in 2019.

```{r, eval = TRUE}
read_rds("num_patents_2019.rds")
```

## Part 3 -  Innovation in Tech: 
What is the most innovative tech sector? For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?
```{r, eval = FALSE}
# for this part of the challenge, we are going to need another data set "uspc.tsv"
# prepare column names and type
col_types <- list(
  uuid = col_character(),
  patent_id = col_character(),
  mainclass_id = col_character(),
  subclass_id = col_character(),
  sequence = col_double()

)

# import data set
uspc_tbl <- vroom(
            file       = "uspc.tsv", 
            delim      = "\t", 
            col_types  = col_types,
            na         = c("", "NA", "NULL")
        )
```

We now reference a copy of the merged table, done for the first step of this challenge. We build a table that contains the organization name, the patent_id and the mainclass_id.

```{r, eval = FALSE}
# set them as data.table
setDT(uspc_tbl)
setDT(org_pat_tbl_copy)

# we first collect only the important part of the org_pat_tbl_copy table, namely the organization name and the patent_id.
org_pat_tbl_copy <- org_pat_tbl_copy[!is.na(location_id), .(patent_id, organization)]

# we now filter the uspc table to get just the patent_id and the mainclass_id

uspc_pat_main_id_tbl <- uspc_tbl[, .(patent_id, mainclass_id)]

```

We now merge the two tables based on the patent_id
```{r, eval = FALSE}
# Create a new table by merging the existing tables through the commnon column "patent_id"
comb_patent_mainclass_tbl <- org_pat_tbl_copy %>%
  left_join(uspc_pat_main_id_tbl, by = "patent_id")

# filter out those companies that do not have a mainclass_id assigned
comb_patent_mainclass_tbl <- comb_patent_mainclass_tbl[!is.na(mainclass_id), 
                                                       .(mainclass_id), by = organization]

```

We can refer to the first part of the challenge and only select the companies among the top 10 companies in total number of patents.

```{r, eval = FALSE}
library(tidyr)
# We gather the top 10 companies in total number of patents from part one of the challenge
top_10_pat_org_copy <- top_10_pat_org[,.(organization)]

# we now merge this table above with the mainclass table and filter out the na values. This is done by using the key method. We then count the number of times a given patent mainclass_id appeared in these top 10 companies.

setkey(comb_patent_mainclass_tbl, organization)
setkey(top_10_pat_org_copy, organization)

top_10_main_id <- comb_patent_mainclass_tbl[top_10_pat_org_copy, on = "organization"][ , .N, by = mainclass_id
][
  order(N, decreasing = TRUE)
][
  1:5, .(`Top 5 USPTO Main Class among the Top 10 Companies` = mainclass_id, `Number of Patents of this Class` = N)
]

write_rds(top_10_main_id, "top_10_mainclass.rds")
```

We print the results showing the top 5 main classes Ids from the top 10 companies in the world based on number of patents issued.
```{r, eval = TRUE}
read_rds("top_10_mainclass.rds")
```


# Challenge 5 - Data Visualization

```{r, eval = TRUE}

```
